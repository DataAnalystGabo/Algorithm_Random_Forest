{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Random Forest**\n",
    "\n",
    "---\n",
    "\n",
    "Random Forest es un **algoritmo de aprendizaje automático** de uso común, registrado por Leo Breiman y Adele Cutler, que combina el resultado de múltiples árboles de decisión para llegar a un resultado único. Su facilidad de uso y flexibilidad han impulsado su adopción, ya que maneja problemas de clasificación y regresión. [1]\n",
    "\\\n",
    "\\\n",
    "Pero, ¿qué son los árboles de decisión? Son estructuras algorítmicas que se inician con una pregunta. A partir de ahí, pueden subdividirse en una secuencia de preguntas para determinar una respuesta de tipo binario.\n",
    "\\\n",
    "\\\n",
    "Estas preguntas constituyen los nodos de decisión en el árbol, que funcionan como un medio para dividir los datos. Cada pregunta ayuda a determinar una decisión final denominada \"nodo hoja\" o \"leaf node\".\n",
    "\\\n",
    "\\\n",
    "Con el objetivo de profundizar en el entendimiento de los conceptos matemáticos, estadísticos, probabilísticos, computacionales y de programación que subyacen al algoritmo de Random Forest, he asumido la enorme empresa de programarlo desde sus cimientos hasta su puesta en funcionamiento, para finalmente comparar su rendimiento con lo ofrecido por una de las librerías más populares de Machine Learning: Scikit-learn.\n",
    "\\\n",
    "\\\n",
    "A continuación se presenta el roadmap de este proyecto:\n",
    "\n",
    "```mermaid\n",
    "flowchart LR;\n",
    "    A[Algoritm Random Forest] --> B[Decision Tree] --> E[\"TreeNode() - Unidad básica\"];\n",
    "\tB[Decision Tree] --> F[\"entropy(y) - Mide la impureza\"];\n",
    "\tB[Decision Tree] --> G[\"find_best_split(x, y) - Busca la mejor división\"];\n",
    "\tB[Decision Tree] --> H[\"split_data(X, y, feature, threshold) - Divide datos\"];\n",
    "\tB[Decision Tree] --> I[\"build_tree(X, y) - Crea el árbol\"];\n",
    "\tB[Decision Tree] --> J[\"predict_tree(tree, X) - Predicción del árbol\"];\n",
    "\tB[Decision Tree] --> K[\"DecisionTreeClassifier - Interface\"];\n",
    "\tA[Algoritm Random Forest] --> C[Random Forest Classifier];\n",
    "\tC[Random Forest Classifier] --> L[\"bootstrap_sample(X, y) - Genera subconjuntos bootstrap\"];\n",
    "\tC[Random Forest Classifier] --> M[\"fit(X, y) - Entrena múltiples árboles\"];\n",
    "\tC[Random Forest Classifier] --> N[\"predict(X) - Votación mayoritaria\"];\n",
    "\tC[Random Forest Classifier] --> Ñ[\"feature_importances - Importancia de variables\"];\n",
    "\tA[Algoritm Random Forest] --> D[Perfomance];\n",
    "\tD[Perfomance] --> O[\"accuracy_score(y_true, y_pred) - Calcula la precisión\"];\n",
    "\tD[Perfomance] --> P[\"confusion_matrix(y_true, y_pred) - Matriz de confusión\"];\n",
    "\tD[Perfomance] --> Q[\"predict(X) -Votación mayoritaria\"];\n",
    "\tD[Perfomance] --> R[\"plot_feature_importance(rf_model) - Visualiza importancia de variables\"];\n",
    "```\n",
    "\n",
    "\\\n",
    "\\\n",
    "[1] IBM, _\"¿Qué es el Random Forest?\"_ [Online]. Available: https://www.ibm.com/mx-es/topics/random-forest. [Accessed: 28-Feb-2025].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **TreeNode:** el átomo de los Árboles de Decisión.\n",
    "\n",
    "---\n",
    "\n",
    "Los Árboles de Decisión son un tipo de algoritmo de aprendizaje supervisado no paramétrico. Están compuestos por una estructura jerárquica de árbol, que consta de un nodo raíz, ramas, nodos internos y nodos hoja. [2]\n",
    "\n",
    "Los nodos representan el punto de decisión en donde el algoritmo debe optar por un camino. El **nodo raíz** es el subconjunto de datos inicial. Los **nodos internos** de decisión son aquellos estadios intermedios que permiten ramificar aún más la toma de decisiones. Finalmente, cuando el algoritmo obtiene un valor de predicción y detiene la división del subconjunto de datos, se lo considera un **nodo hoja**.\n",
    "\n",
    "**TreeNode** es la clase que abstrae las características de los nodos y nos permite instanciar los distintos tipos. A continuación, se describen sus parámetros:\n",
    "\n",
    "-   **`feature_index`:** índice de la variable utilizada para dividir el nodo.\n",
    "-   **`threshold`:** valor umbral para la división.\n",
    "-   **`left`:** nodo hijo izquierdo.\n",
    "-   **`right`:** nodo hijo derecho.\n",
    "-   **`value`:** valor predictorio para los nodos hoja.\n",
    "\n",
    "[2] IBM, \"¿Qué es un árbol de decisión?\" [Online]. Available: https://www.ibm.com/es-es/think/topics/decision-trees. [Accessed: 28-Feb-2025].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_decision.TreeNode import TreeNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_node = TreeNode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_node = TreeNode(feature_index=2, threshold=3.5, left=leaf_node, right=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodo hoja:  {'feature_index': None, 'threshold': None, 'left': None, 'right': None, 'value': None}\n",
      "Nodo interno:  {'feature_index': 2, 'threshold': 3.5, 'left': <tree_decision.TreeNode.TreeNode object at 0x00000183B4CD8C20>, 'right': None, 'value': None}\n"
     ]
    }
   ],
   "source": [
    "print(\"Nodo hoja: \", leaf_node.__dict__)\n",
    "print(\"Nodo interno: \", internal_node.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Mecanismos de decisión**\n",
    "\n",
    "---\n",
    "\n",
    "La entropía es una métrica que nos permite calcular la impureza de un conjunto de datos, es decir, su grado de desorden o incertidumbre. Otra métrica muy utilizada en los algoritmos de Random Forest es la medida de Gini. Sin embargo, para este ejercicio práctico utilizaremos la entropía como mecanismo de decisión.\n",
    "\\\n",
    "\\\n",
    "Para poder comprender en profundidad este concepto supongamos que debemos desarrollar un algoritmo de predicción que pueda detectar cuáles de los emails que nos llegan a nuestra bandeja son spam y cuáles no. Veamos los siguientes escenarios:\n",
    "\n",
    "-   **Escenario 1:** 5 emails son spam y 5 emails no lo son.\n",
    "-   **Escenario 2:** 7 emails son spam y 3 emails no lo son.\n",
    "-   **Escenario 3:** 9 emails son spam y 1 email no lo es.\n",
    "-   **Escenario 4:** Los 10 emails son spam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tree_decision.entropy import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emails spam -> 1\n",
    "# Emails no spam -> 0\n",
    "scenary_one = np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n",
    "scenary_two = np.array([1, 1, 1, 1, 1, 1, 1, 0, 0, 0])\n",
    "scenary_three = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 0])\n",
    "scenary_four = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado los distintos escenarios vamos a calcular la entropía para cada uno de ellos. Su ecuación viene dada por:\n",
    "\n",
    "$$\n",
    "H(x) = - \\sum_{x=0}^{n} p(x) \\log_2(p(x))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_scenary_one = entropy(scenary_one)\n",
    "entropy_scenary_two = entropy(scenary_two)\n",
    "entropy_scenary_three = entropy(scenary_three)\n",
    "entropy_scenary_four = entropy(scenary_four)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos los resultados de la entropía para los diferentes escenarios, podemos analizar qué nos dicen en términos de impureza e incertidumbre de los conjuntos de datos.\n",
    "\n",
    "**Escenario 1:** 5 emails son spam y 5 emails no lo son.\n",
    "\n",
    "-   **Entropía:** `1.0`  \n",
    "    La entropía en este caso es 1.0, lo que indica que el conjunto de datos está completamente equilibrado entre las dos clases (spam y no spam). Esto significa que la incertidumbre es máxima, ya que no hay ninguna preferencia hacia una clase, lo que nos dice que el modelo necesita el máximo de información para hacer una predicción correcta.\n",
    "\n",
    "**Escenario 2:** 7 emails son spam y 3 emails no lo son.\n",
    "\n",
    "-   **Entropía:** `0.881`  \n",
    "    En este escenario, la entropía es de 0.881, lo que es más bajo que el valor de 1.0 del escenario anterior. Aunque hay más emails spam que no spam, aún hay algo de incertidumbre porque no es una clasificación completamente homogénea. La impureza sigue siendo relativamente alta, pero la división de clases es algo más clara.\n",
    "\n",
    "**Escenario 3:** 9 emails son spam y 1 email no lo es.\n",
    "\n",
    "-   **Entropía:** `0.469`  \n",
    "    Aquí la entropía es de 0.469, lo que indica que el conjunto es mucho más homogéneo que en los casos anteriores. La mayoría de los emails son spam, lo que reduce considerablemente la incertidumbre. Aunque no es completamente puro, la impureza ha disminuido, lo que indica que la división de clases es más clara.\n",
    "\n",
    "**Escenario 4:** Los 10 emails son spam.\n",
    "\n",
    "-   **Entropía:** `0.0`  \n",
    "    Finalmente, en este escenario donde todos los emails son spam, la entropía es 0.0. Esto significa que el conjunto es completamente homogéneo, y no hay ninguna incertidumbre ni impureza. Todos los elementos pertenecen a la misma clase, por lo que el modelo no necesita ninguna información adicional para hacer una predicción.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy scenary one:  1.0\n",
      "Entropy scenary two:  0.8812908992306927\n",
      "Entropy scenary three:  0.4689955935892812\n",
      "Entropy scenary four:  -0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Entropy scenary one: \", entropy_scenary_one)\n",
    "print(\"Entropy scenary two: \", entropy_scenary_two)\n",
    "print(\"Entropy scenary three: \", entropy_scenary_three)\n",
    "print(\"Entropy scenary four: \", entropy_scenary_four)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **La forma más eficiente de dividir los datos**\n",
    "\n",
    "---\n",
    "\n",
    "Los Árboles de Decisión dividen y ramifican un conjunto de datos con el objetivo de llegar a una conclusión final. Pero, ¿cuál es la manera más eficiente y efectiva de realizar esta segmentación? ¿Bajo qué criterio se dividen los datos?\n",
    "\n",
    "La función `find_best_split(X, y)` implementa un procedimiento para determinar el umbral óptimo de división dentro de un conjunto de datos.  \n",
    "Recibe dos argumentos:\n",
    "\n",
    "-   **X**: Matriz de características (variables independientes) del dataset.\n",
    "-   **y**: Vector objetivo que contiene las etiquetas o valores a predecir.\n",
    "\n",
    "Este método sigue **ocho pasos fundamentales**, que se detallan a continuación:\n",
    "\n",
    "1. **Inicializa un diccionario `results`**, que almacenará los umbrales evaluados junto con la entropía total correspondiente.\n",
    "2. **Recorre cada una de las características (columnas) de X**, iterando sobre todas las variables disponibles.\n",
    "3. **Ordena los valores de cada característica de menor a mayor** para evaluar posibles puntos de corte.\n",
    "4. **Calcula los valores intermedios entre cada par consecutivo de datos ordenados** y los utiliza iterativamente como posibles _thresholds_ (umbrales) de división.\n",
    "5. **Divide el conjunto de datos** en dos subconjuntos:\n",
    "    - **`left_y`**: Conjunto de valores objetivo asociados a observaciones menores o iguales al umbral.\n",
    "    - **`right_y`**: Conjunto de valores objetivo asociados a observaciones mayores al umbral.\n",
    "6. **Calcula la entropía de cada subconjunto**, una medida del grado de desorden o impureza en los datos resultantes de la división.\n",
    "7. **Calcula la entropía total**, ponderando la entropía de cada subconjunto en función de la cantidad de elementos que contiene respecto al total de datos en `X`. Luego, almacena en `results` el umbral evaluado junto con su respectiva entropía total.\n",
    "8. **Devuelve el valor del umbral (threshold) que minimiza la entropía total**, es decir, aquel que genera la división más homogénea posible dentro del conjunto de datos.\n",
    "\n",
    "Este proceso garantiza que la segmentación de los datos sea lo más informativa posible, favoreciendo la creación de nodos homogéneos dentro del Árbol de Decisión.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_decision.find_best_split import find_best_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejemplo de división de datos con `find_best_split`**\n",
    "\n",
    "---\n",
    "\n",
    "Para este ejemplo, tenemos dos arreglos:\n",
    "\n",
    "-   **`X`**: Una matriz que representa las características o variables independientes del conjunto de datos, el cual agrupa emails clasificados como **spam (1)** o **no spam (0)**.\n",
    "-   **`y`**: Un vector que contiene los valores objetivo (_target_), indicando si un email es spam (`1`) o no (`0`).\n",
    "\n",
    "Cada columna de la matriz `X` representa una característica específica del contenido del email:\n",
    "\n",
    "1. **Primera columna**: Cantidad de veces que aparece la frase `\"Sólo por hoy\"`.\n",
    "2. **Segunda columna**: Cantidad de signos de exclamación (`!`).\n",
    "3. **Tercera columna**: Cantidad de veces que aparece la palabra `\"Oferta\"`.\n",
    "\n",
    "El conjunto de datos es el siguiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 5, 1], [3, 2, 2], [4, 6, 1], [6, 8, 3], [1, 5, 4]])\n",
    "y = np.array([1, 0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ejecutar la función `find_best_split`, obtenemos una tupla con tres valores:\n",
    "\n",
    "**Índice de la mejor característica (feature):** Indica qué columna de X proporciona la mejor división.  \n",
    "**Umbral óptimo (threshold):** Punto de corte que genera la separación más homogénea del conjunto de datos.  \n",
    "**Entropía total:** Medida de la impureza resultante tras la división de los datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature, threshold, entropy = find_best_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature (índice):  0\n",
      "threshold (umbral de corte):  1.0\n",
      "entropy (entropía ponderada):  0.5509775004326937\n"
     ]
    }
   ],
   "source": [
    "print(\"feature (índice): \", feature)\n",
    "print(\"threshold (umbral de corte): \", threshold)\n",
    "print(\"entropy (entropía ponderada): \", entropy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "random_forest_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
